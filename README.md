# Dataset Transformation
Production-ready Phase 1 pipeline that converts the Hugging Face dataset `HeshamHaroon/Arabic_Function_Calling` into Mobile Actions / FunctionGemma chat format.

## Purpose
- Convert Arabic function-calling data into strict chat JSONL compatible with `tokenizer.apply_chat_template`.
- Preserve Arabic text and tool arguments verbatim; never hallucinate missing fields.
- Auto-build a JSON Schema registry for every unique `function_name`.
- Validate the final JSONL so it is ready for Phase 2 fine-tuning.

## Dataset Source
- Hugging Face: `HeshamHaroon/Arabic_Function_Calling` (train split by default).
- Columns are detected automatically; currently exposed: `query_ar`, `query_en`, `function_name`, `arguments`, `dialect`, `domain`, `requires_function`.
- A raw snapshot is written to `raw/dataset.jsonl` for reproducibility.

## Repository Layout
- `scripts/convert_dataset.py` — load HF dataset, log column/sample snapshot, write raw copy, convert to chat format, and generate `schemas/registry.json`.
- `scripts/validate_with_tokenizer.py` — replay each record through `AutoTokenizer.apply_chat_template` (defaults to `google/function-gemma-2b`).
- `raw/` — raw dataset snapshot (`dataset.jsonl`).
- `output/` — converted dataset (`dataset_functiongemma.jsonl`).
- `schemas/` — function registry keyed by function name.

## Output Format
Case 1 — Tool call required
```json
{
  "messages": [
    { "role": "user", "content": "<arabic query>" },
    { "role": "assistant", "tool_calls": [
      { "name": "<function_name>", "arguments": { ... } }
    ]}
  ]
}
```
Case 2 — No tool call
```json
{
  "messages": [
    { "role": "user", "content": "<arabic query>" },
    { "role": "assistant", "content": "<arabic answer>" }
  ]
}
```

### Real example (row 0 from the train split)
```json
{
  "messages": [
    { "role": "user", "content": "أريد إنشاء تذكير بعنوان اجتماع عمل في الساعة الثالثة ظهراً يوم ١٥ أكتوبر" },
    { "role": "assistant", "tool_calls": [
      { "name": "set_reminder", "arguments": { "datetime": "2023-10-15T15:00:00", "title": "اجتماع عمل" } }
    ]}
  ]
}
```

## Run the conversion
```bash
python scripts/convert_dataset.py \
  --dataset HeshamHaroon/Arabic_Function_Calling \
  --split train \
  --raw-path raw/dataset.jsonl \
  --output-path output/dataset_functiongemma.jsonl \
  --registry-path schemas/registry.json
```
Useful options:
- `--max-rows` to cap processed rows.
- `--query-field` / `--answer-field` to override auto-detection.

Notes:
- The script logs detected columns and the first three rows for inspection.
- Rows without a user query or without either `function_name` or `answer` are skipped; totals are reported at the end.

## Run the validation
- Requires access to the tokenizer for `google/function-gemma-2b` (model is gated; set `HUGGINGFACE_TOKEN` or run `huggingface-cli login` first).
```bash
python scripts/validate_with_tokenizer.py \
  --dataset-path output/dataset_functiongemma.jsonl \
  --model google/function-gemma-2b
```
Flags: `--fail-fast` to stop on first failure, `--max-records` for spot checks.

## Team contribution rules
- Keep changes small and reversible; prefer additive diffs.
- Do not overwrite raw or output files generated by others without coordination.
- Maintain deterministic conversion (no shuffling or nondeterministic ordering).
- Add validation runs or tests with command outputs when changing logic.
- Document any new parameters or behaviors in this README.
